{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Encoder/Decoder Classifier\n",
        "## CS 72 Final Project\n",
        "### John Guerrerio\n",
        "### john.j.guerrerio.26@dartmouth.edu\n",
        "\n",
        "This notebook contains the code to train an encoder/decoder model Inbox Guardian classification task.  We use DistiliBERT as our encoder and two linear layers as our decoder.  We use a dataset of the most recent 500 email chains we recieved - see the write up for details.  \\\n",
        "\\\n",
        "Note: This code requires a gpu to run in a reasonable amount of time"
      ],
      "metadata": {
        "id": "cFsihiP7eWGh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsfmvkqRD_00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import tensor\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import sys\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23rAW-WVEIOI"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42 # random seed to ensure results are reproducible\n",
        "META = False # True if we only want to show the model senders and subjects, false if we want to pass in email body as well\n",
        "THREE_CLASSES = False # True for trinary classification task, false for binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiAaWviIERqj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('fullDataset.csv')\n",
        "# Shuffles the dataset, as it was ordered by label during construction\n",
        "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqzxCvg_Gw_6"
      },
      "outputs": [],
      "source": [
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9trfbyaFuWo"
      },
      "outputs": [],
      "source": [
        "# print graph of number of documents for each label\n",
        "df['Label'].plot(kind='hist', bins=20, title='Label')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHXCQaFREdmz"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased') # load the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-lghMaqK-1X"
      },
      "outputs": [],
      "source": [
        "# Converts all \"urgent\" labels to \"relevant\"\n",
        "# Allows us to collpase the trinary classification task into a relevant/irrelevant binary classification task\n",
        "def changeLabels(x):\n",
        "  if x == 2:\n",
        "    return 1\n",
        "  else:\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBOCdbkHEeLb"
      },
      "outputs": [],
      "source": [
        "# define our docs based on the META flag\n",
        "if META:\n",
        "  docs = df[\"Meta\"].tolist()\n",
        "else:\n",
        "  docs = df[\"Full\"].tolist()\n",
        "\n",
        "# define our labels based on the THREE_CLASSES flags\n",
        "if THREE_CLASSES:\n",
        "  labels = df[\"Label\"].tolist()\n",
        "else:\n",
        "  labels = df['Label'].apply(changeLabels).tolist()\n",
        "\n",
        "print(len(docs))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xICKuId-GGG0"
      },
      "outputs": [],
      "source": [
        "# shuffles the data and splits it into the train, test, and validation sets\n",
        "train, validAndTest, trainLabels, validAndTestLabels = train_test_split(docs, labels, test_size=0.3, random_state=RANDOM_STATE)\n",
        "valid, test, validLabels, testLabels = train_test_split(validAndTest, validAndTestLabels, test_size=0.5, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT5U4Qa6Gg8T"
      },
      "outputs": [],
      "source": [
        "# tokenize the product names - turns them into a format the BERT model can understand\n",
        "trainTokenized = tokenizer(train, padding='max_length', max_length = 512, truncation=True, return_tensors='pt', return_attention_mask = True)\n",
        "validTokenized = tokenizer(valid, padding='max_length', max_length = 512, truncation=True, return_tensors='pt', return_attention_mask = True)\n",
        "testTokenized = tokenizer(test, padding='max_length', max_length = 512, truncation=True, return_tensors='pt', return_attention_mask = True)\n",
        "\n",
        "# need to keep track of attention masks for each document so our model ignores padding tokens properly\n",
        "trainTokens = trainTokenized[\"input_ids\"]\n",
        "print(trainTokens[0].size())\n",
        "trainMask = trainTokenized[\"attention_mask\"]\n",
        "print(trainMask[0].size())\n",
        "\n",
        "validTokens = validTokenized[\"input_ids\"]\n",
        "validMask = validTokenized[\"attention_mask\"]\n",
        "\n",
        "testTokens = testTokenized[\"input_ids\"]\n",
        "testMask = testTokenized[\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZwQoFRUGtqM"
      },
      "outputs": [],
      "source": [
        "print(len(train))\n",
        "print(len(trainLabels))\n",
        "print()\n",
        "\n",
        "print(len(valid))\n",
        "print(len(validLabels))\n",
        "print()\n",
        "\n",
        "print(len(test))\n",
        "print(len(testLabels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKE9GXrPGz-m"
      },
      "outputs": [],
      "source": [
        "# a class to represent the train, validation, and test sets\n",
        "# the Dataset class handles dividing the data into minibatches and producing the minibatches for us\n",
        "class emailDataset(Dataset):\n",
        "  def __init__(self, data, labels, mask):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "    self.mask = mask\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.labels[idx], self.mask[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqazl7OKHd27"
      },
      "outputs": [],
      "source": [
        "# build the dataset objects for train, validation, and test sets\n",
        "trainData = emailDataset(trainTokens, trainLabels, trainMask)\n",
        "validData = emailDataset(validTokens, validLabels, validMask)\n",
        "testData = emailDataset(testTokens, testLabels, testMask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r3OHAuwHeL9"
      },
      "outputs": [],
      "source": [
        "# datloader object for test set\n",
        "testLoader = DataLoader(testData, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrNzy3TxHyXc"
      },
      "outputs": [],
      "source": [
        "# class that defines model architecture for us\n",
        "class BertEncoderDecoder(nn.Module):\n",
        "    def __init__(self, DROPOUT, dbert):\n",
        "        super(BertEncoderDecoder, self).__init__()\n",
        "        # BERT encoder\n",
        "        self.dbert = dbert\n",
        "        # Decoder\n",
        "        self.dropout = nn.Dropout(p=DROPOUT)\n",
        "        self.linear1 = nn.Linear(768,64)\n",
        "        self.ReLu = nn.ReLU()\n",
        "        # classification head depends on how many classes we have\n",
        "        if THREE_CLASSES:\n",
        "          self.linear2 = nn.Linear(64,3)\n",
        "        else:\n",
        "          self.linear2 = nn.Linear(64,2)\n",
        "\n",
        "    def forward(self, tokens, mask):\n",
        "        x = self.dbert(input_ids=tokens, attention_mask=mask)\n",
        "        x = x[\"last_hidden_state\"][:,0,:] # we use the last hidden state of BERT as our context vector\n",
        "        x = self.dropout(x) # dropout on BERT output, prevents overfitting\n",
        "        x = self.linear1(x)\n",
        "        x = self.ReLu(x)\n",
        "        logits = self.linear2(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_1E4CdAXpL4"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training - THIS CODE WORKS BEST ON A GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # need to change options to train on GPU\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxHspKbjIY4i"
      },
      "outputs": [],
      "source": [
        "# list of possible values for hyperparameters - we will grid search across all of these values\n",
        "\n",
        "DROPOUT = [0.25, 0.2, 0.15] # dropout probability\n",
        "ATTN_DROPOUT = 0.2 # dropout probability for the attention equation\n",
        "EPOCHS = [4, 8, 16] # number of epochs to train for\n",
        "LEARNING_RATE = [0.002, 0.001, 0.005, 0.0001, 0.0005] # learning rate\n",
        "BATCH_SIZE = [8, 4, 16] # batch size\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # softmax and loss for classification layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnXnkRIkgj_r"
      },
      "outputs": [],
      "source": [
        "def setup(batchSize, dropout, atnDropout, lr):\n",
        "    \"\"\"\n",
        "    This function returns the necessary objects we need to train a model with a given set of hyperparameters\n",
        "\n",
        "    Args:\n",
        "      batchSize: The batch size to use when training\n",
        "      dropout: The dropout probability to use when training\n",
        "      atnDropout: The attention dropout probability to use when training\n",
        "      lr: The learning rate to use when training\n",
        "    Returns:\n",
        "      trainLoader: A dataloader object for the train dataset constructed using the given batch size\n",
        "      validLoader: A dataloader object for the validation dataset constructed using the given batch size\n",
        "      classifier: A model object constructed using the given hyperparameters\n",
        "      optimizer: An Adam optimizer object constructed using the given hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    # build the dataloader objects for train, validation sets\n",
        "    trainLoader = DataLoader(trainData, batch_size=batchSize)\n",
        "    validLoader = DataLoader(validData, batch_size=batchSize)\n",
        "\n",
        "    # build distilibert object\n",
        "    dbert = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased', dropout=dropout, attention_dropout=atnDropout)\n",
        "\n",
        "    # build model object and send it to the GPU\n",
        "    classifier = BertEncoderDecoder(dropout, dbert).to(device)\n",
        "\n",
        "    # build the optimizer object\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr = lr)\n",
        "\n",
        "    # freeze DistiliBERT parameters to avoid overfitting\n",
        "    for param in classifier.dbert.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    return trainLoader, validLoader, classifier, optimizer\n",
        "\n",
        "def train(trainLoader):\n",
        "    \"\"\"\n",
        "    This function handles a single training loop\n",
        "\n",
        "    Args:\n",
        "      trainLoader: A dataloader object for the training set\n",
        "    Returns\n",
        "      The macro-averaged f1 of the current epoch\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = []\n",
        "    groundTruth = []\n",
        "\n",
        "    classifier.train() # activated dropout\n",
        "\n",
        "    for description, labels, mask in tqdm(trainLoader):\n",
        "      # send minibatch to gpu for efficient training\n",
        "      description = description.to(device)\n",
        "      labels = labels.to(device)\n",
        "      mask = mask.to(device)\n",
        "\n",
        "      # Get prediction & loss\n",
        "      prediction = classifier(description, mask)\n",
        "      loss = criterion(prediction, labels)\n",
        "\n",
        "      # determine the optimal direction to increment parameters\n",
        "      loss.backward()\n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # zero the gradient so we don't accumulate optimizer steps\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # get model prediction\n",
        "      prediction_index = prediction.argmax(axis=1)\n",
        "\n",
        "      predictions += prediction_index.tolist()\n",
        "      groundTruth += labels.tolist()\n",
        "\n",
        "    return f1_score(groundTruth, predictions, average=\"macro\")\n",
        "\n",
        "\n",
        "def evaluate(validLoader):\n",
        "  \"\"\"\n",
        "  This function evalautes a trained model on the validation set - we use this function to determine how well a given set of hyperparameters perform\n",
        "\n",
        "  Args:\n",
        "    validLoader: A dataloader object for the validation set\n",
        "  Returns:\n",
        "    The macro-averaged f1 of the model on the validation set\n",
        "  \"\"\"\n",
        "  predictions = []\n",
        "  groundTruth = []\n",
        "\n",
        "  classifier.eval() # turn off dropout for evaluation\n",
        "  with torch.no_grad(): # turn off gradient calculation so we don't train on the validation set\n",
        "    for description, labels, mask in validLoader:\n",
        "      # send minibatch to gpu for efficient calcuations\n",
        "      description = description.to(device)\n",
        "      labels = labels.to(device)\n",
        "      mask = mask.to(device)\n",
        "\n",
        "      # get model prediction\n",
        "      prediction = classifier(description, mask)\n",
        "      prediction_index = prediction.argmax(axis=1)\n",
        "\n",
        "      predictions += prediction_index.tolist()\n",
        "      groundTruth += labels.tolist()\n",
        "\n",
        "  return f1_score(groundTruth, predictions, average=\"macro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVDUUBMxI2x7"
      },
      "outputs": [],
      "source": [
        "# this cell performs a grid search across all combinations of the hyperparameter values we define\n",
        "\n",
        "# best f1 and best hperparameter values we have seen so far\n",
        "bestf1 = 0\n",
        "bestParams = [0, 0, 0, 0]\n",
        "\n",
        "for dropout in DROPOUT:\n",
        "  for epoch in EPOCHS:\n",
        "    for lr in LEARNING_RATE:\n",
        "      for batch in BATCH_SIZE:\n",
        "        print(\"Dropout probability: \" + str(dropout))\n",
        "        print(\"Epochs: \" + str(epoch))\n",
        "        print(\"Learning rate: \" + str(lr))\n",
        "        print(\"Batch size: \" + str(batch))\n",
        "\n",
        "        trainLoader, validLoader, classifier, optimizer = setup(batch, dropout, ATTN_DROPOUT, lr)\n",
        "\n",
        "        # training loop\n",
        "        for e in range(epoch):\n",
        "          trainf1 = train(trainLoader)\n",
        "          print(f'Epoch {e+1} Macro-Averaged F1: {trainf1}')\n",
        "\n",
        "\n",
        "        evalf1 = evaluate(validLoader)\n",
        "        print(f'Validation Macro-Averaged F1: {evalf1}')\n",
        "\n",
        "        # update if we see a better f1\n",
        "        if evalf1 > bestf1:\n",
        "          bestParams = [dropout, epoch, lr, batch]\n",
        "          bestf1 = evalf1\n",
        "        print(\"----------------------\")\n",
        "\n",
        "print(f'Best f1: {bestf1}')\n",
        "print(f'Best dropout: {bestParams[0]}')\n",
        "print(f'Best num epochs: {bestParams[1]}')\n",
        "print(f'Best learning rate: {bestParams[2]}')\n",
        "print(f'Best batch size: {bestParams[3]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63qDxHbnWMNk"
      },
      "outputs": [],
      "source": [
        "# dictionary to store information about the training of the best performing model - will be useful to generate graphs\n",
        "history = {}\n",
        "history[\"epoch\"]=[]\n",
        "history[\"train_loss\"]=[]\n",
        "history[\"valid_loss\"]=[]\n",
        "history[\"train_accuracy\"]=[]\n",
        "history[\"valid_accuracy\"]=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iXXduaYpDjoO"
      },
      "outputs": [],
      "source": [
        "# this cell trains a model using the best performing hyperparameters we found\n",
        "# we keep track of more information in each epoch and evaluate on the validation set at the end of each epoch\n",
        "\n",
        "trainLoader, validLoader, classifier, optimizer = setup(bestParams[3], bestParams[0], ATTN_DROPOUT, bestParams[2])\n",
        "\n",
        "for e in range(bestParams[1]):\n",
        "  classifier.train() # activated dropout\n",
        "\n",
        "  train_loss = 0.0\n",
        "  train_accuracy = []\n",
        "\n",
        "  # loop over each minibatch\n",
        "  for description, labels, mask in tqdm(trainLoader):\n",
        "\n",
        "      # send minibatch to gpu for efficient training\n",
        "      description = description.to(device)\n",
        "      labels = labels.to(device)\n",
        "      mask = mask.to(device)\n",
        "\n",
        "      # Get prediction & loss\n",
        "      prediction = classifier(description, mask)\n",
        "      loss = criterion(prediction, labels)\n",
        "\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # determine the optimal direction to increment parameters\n",
        "      loss.backward()\n",
        "\n",
        "      # update parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # zero the gradient so we don't accumulate optimizer steps\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # get model predictions\n",
        "      prediction_index = prediction.argmax(axis=1)\n",
        "\n",
        "      # determine which minibatch documents were classified correctly\n",
        "      accuracy = (prediction_index==labels)\n",
        "      train_accuracy += accuracy\n",
        "\n",
        "  # calculate train accuracy\n",
        "  train_accuracy = (sum(train_accuracy) / len(train_accuracy))\n",
        "\n",
        "  classifier.eval() # turn off dropout for evaluation\n",
        "  valid_loss = 0.0\n",
        "  valid_accuracy = []\n",
        "\n",
        "  # evaluate on the validation set\n",
        "  with torch.no_grad(): # turn off gradient calculation so we don't train on the validation set\n",
        "    for description, labels, mask in validLoader:\n",
        "      # send minibatch to gpu for efficient calcualtion\n",
        "      description = description.to(device)\n",
        "      labels = labels.to(device)\n",
        "      mask = mask.to(device)\n",
        "\n",
        "      # get model predictions and loss\n",
        "      prediction = classifier(description, mask)\n",
        "      loss = criterion(prediction, labels)\n",
        "\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "      # get model prediction categories\n",
        "      prediction_index = prediction.argmax(axis=1)\n",
        "\n",
        "      # calculate accuracy\n",
        "      accuracy = (prediction_index==labels)\n",
        "      valid_accuracy += (accuracy)\n",
        "\n",
        "  valid_accuracy = (sum(valid_accuracy) / len(valid_accuracy)) # sum sums up the boolean tensors, which themselves have a method to sum up\n",
        "\n",
        "  # keep a record of our training results\n",
        "  history[\"epoch\"].append(e+1)\n",
        "  history[\"train_loss\"].append(train_loss / len(trainLoader))\n",
        "  history[\"valid_loss\"].append(valid_loss / len(validLoader))\n",
        "  history[\"train_accuracy\"].append(train_accuracy.tolist())\n",
        "  history[\"valid_accuracy\"].append(valid_accuracy.tolist())\n",
        "\n",
        "  # output results\n",
        "  print(f'Epoch {e+1}')\n",
        "  print(f'\\t\\t Training Loss: {train_loss / len(trainLoader) :10.3f} \\t\\t Validation Loss: {valid_loss / len(validLoader) :10.3f}')\n",
        "  print(f'\\t\\t Training Accuracy: {train_accuracy :10.3%} \\t\\t Validation Accuracy: {valid_accuracy :10.3%}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI3hO71YPm7U"
      },
      "outputs": [],
      "source": [
        "# graphs of train/validation accuracy over each epoch and train/validation loss over each epoch\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "ax[0].set(title='Loss')\n",
        "ax[0].plot(history['train_loss'], label='Training')\n",
        "ax[0].plot(history['valid_loss'], label='Validation')\n",
        "ax[0].legend(loc=\"upper right\")\n",
        "\n",
        "ax[1].set(title='Accuracy')\n",
        "ax[1].plot(history['train_accuracy'], label='Training')\n",
        "ax[1].plot(history['valid_accuracy'], label='Validation')\n",
        "ax[1].legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drD1u1VtPSqL"
      },
      "outputs": [],
      "source": [
        "# evaluate the model with the bext-performing hyperparameters on the test set\n",
        "\n",
        "classifier.eval() # turn off dropout for evaluation\n",
        "\n",
        "groundTruth = np.zeros(len(testLoader)) # holds the labels for the test set\n",
        "predictions = np.zeros(len(testLoader)) # holds the model's predictions on the test set\n",
        "\n",
        "i = 0\n",
        "with torch.no_grad(): # turn off gradient calculation so we don't train on the test set\n",
        "  for description, label, mask in tqdm(testLoader):\n",
        "    # send minibatch to gpu for efficient calculation\n",
        "    description = description.to(device)\n",
        "    label = label.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    # get prediction probabilites and predicted class\n",
        "    prediction = classifier(description, mask)\n",
        "    predictedClass = int(prediction.argmax(axis=1).item())\n",
        "    predictions[i] = predictedClass\n",
        "\n",
        "    # get ground truth label for a document\n",
        "    goldClass = int(label.item())\n",
        "    groundTruth[i] = goldClass\n",
        "\n",
        "    i+= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBuh09PkPX_e"
      },
      "outputs": [],
      "source": [
        "print(classification_report(groundTruth, predictions)) # print precision, recall, and f1 for each class and overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLTYSh11PZut"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(groundTruth, predictions)) # print confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyUvrioUQr7s"
      },
      "outputs": [],
      "source": [
        "# This cell is only necessary if you are running this notebook in Colab\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlyGeIsGQt3B"
      },
      "outputs": [],
      "source": [
        "# save the model to be used later for inference\n",
        "path = \"/content/gdrive/MyDrive/ColabOutput/BERTBinaryEmails.pth\"\n",
        "torch.save(classifier.state_dict(), path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}